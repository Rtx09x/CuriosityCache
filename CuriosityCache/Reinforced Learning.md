
---

# DeepSeek R1: A Deep Dive into Reinforcement Learning for Reasoning in Large Language Models

The artificial intelligence landscape has witnessed a rapid evolution, with large language models (LLMs) demonstrating remarkable capabilities in various natural language processing tasks. Among these advancements, the release of DeepSeek R1 has garnered significant attention within the AI community. This model has been reported to achieve performance levels comparable to those of established models like OpenAI's o1, particularly in the domain of reasoning tasks, while offering notable advantages in terms of cost-efficiency and open-source availability. A crucial aspect of DeepSeek R1's development lies in its extensive utilization of reinforcement learning (RL) to enhance the logical thinking and problem-solving abilities of the model. This report aims to provide a comprehensive analysis of the reinforcement learning methodologies employed in the training of DeepSeek R1, with a specific focus on the Group Relative Policy Optimization (GRPO) algorithm and the intricate multi-stage training pipeline implemented by DeepSeek AI.

## Understanding DeepSeek R1 and its Predecessor

DeepSeek AI has introduced its first generation of reasoning models, namely DeepSeek R1-Zero and DeepSeek R1, with a clear emphasis on leveraging reinforcement learning to significantly improve the reasoning capabilities inherent in large language models. In an initial experimental phase, the DeepSeek team developed DeepSeek R1-Zero, a model trained purely through reinforcement learning without any preceding supervised fine-tuning. This approach was designed to explore the potential for emergent reasoning behaviors to arise solely from RL interactions. Building upon the insights gained from this initial experiment, DeepSeek R1 was developed using a more sophisticated multi-stage training strategy that integrates supervised fine-tuning alongside reinforcement learning to achieve even greater performance.

The architecture underpinning DeepSeek R1 is a Mixture of Experts (MoE) framework, comprising a total of 671 billion parameters. However, during the processing of each token, only a fraction of these parameters, specifically 37 billion, are activated. This design choice allows for a dynamic allocation of computational resources, contributing to a balance between the model's performance and its efficiency. The foundational model upon which both DeepSeek R1-Zero and DeepSeek R1 were built is DeepSeek-V3-Base.

A key distinction between the two models lies in their training methodologies. DeepSeek R1-Zero was trained exclusively using large-scale reinforcement learning (RL) without any preliminary supervised fine-tuning (SFT). This represented an attempt to ascertain whether advanced reasoning capabilities could be cultivated in an LLM through RL alone, without relying on pre-labeled data to guide the learning process for reasoning patterns. In contrast, DeepSeek R1 employs a more involved multi-stage training pipeline. This pipeline commences with supervised fine-tuning on a carefully selected, small dataset of examples, referred to as "cold-start data," before the application of reinforcement learning. The inclusion of this initial SFT phase in DeepSeek R1 was a direct response to the limitations observed in DeepSeek R1-Zero, particularly issues related to the readability and the mixing of languages in the generated text.

|                         |                                       |                                                                        |
| ----------------------- | ------------------------------------- | ---------------------------------------------------------------------- |
| **Feature**             | **DeepSeek R1-Zero**                  | **DeepSeek R1**                                                        |
| Initial Training        | None                                  | Supervised Fine-Tuning (Cold Start Data)                               |
| Primary RL Algorithm    | GRPO                                  | GRPO                                                                   |
| Reasoning Focus         | Yes                                   | Yes                                                                    |
| Language Consistency    | Not explicitly incentivized initially | Incentivized with a specific reward                                    |
| Final Alignment         | Accuracy and Format Rewards           | Accuracy, Format, and Human Preferences (Helpfulness and Harmlessness) |
| Readability & Coherence | Lower                                 | Higher                                                                 |

The contrasting training approaches of DeepSeek R1-Zero and DeepSeek R1 offer a significant case study in the effects of integrating supervised fine-tuning into a reinforcement learning-dominated training strategy for enhancing reasoning in LLMs. The decision to use pure RL for R1-Zero demonstrated the potential for reasoning skills to emerge; however, this came at the expense of user-friendliness, specifically in terms of readability and language coherence. This directly influenced the development of R1, where an initial SFT stage was incorporated to mitigate these issues. This evolution in training methodology underscores the inherent trade-offs between allowing a model to autonomously develop its reasoning abilities through RL and actively guiding its learning with high-quality supervised data to ensure practical usability in real-world applications.

## The Foundation: Reinforcement Learning for Reasoning in LLMs

Reinforcement learning (RL) represents a paradigm within machine learning where an agent learns to optimize its behavior in an environment by maximizing the cumulative reward it receives through its actions. In the context of training large language models, RL is particularly valuable for aligning the model's outputs with desired characteristics such as factual correctness, logical consistency, and human-like reasoning processes. Unlike traditional supervised learning, where models learn from explicit input-output pairs, RL enables models to learn through a process of trial and error, refining their internal parameters based on the feedback, or rewards, obtained from the solutions they generate.

However, the application of traditional RL techniques to the complex domain of natural language presents several unique challenges. One significant hurdle is the vast action space inherent in language generation, where the model must choose the next token from a vocabulary that can contain tens of thousands of words. Furthermore, the rewards for generating coherent and logical reasoning are often sparse and can be delayed until the entire reasoning chain or final answer is produced. This makes it difficult for the model to attribute credit to individual steps in the reasoning process. Designing effective reward functions that accurately capture the nuances of desired reasoning behaviors without inadvertently incentivizing undesirable shortcuts or "reward hacking" is also a complex task. Traditional RL frameworks like Proximal Policy Optimization (PPO), while effective in many domains, can be computationally expensive when applied to the scale of modern LLMs, often requiring the training of an additional "critic" network that estimates the value of different actions. Finally, ensuring the stability of the training process for models with billions of parameters, as is the case with DeepSeek R1, is a considerable challenge in itself.

The utilization of reinforcement learning for enhancing reasoning in LLMs represents a notable shift from the more conventional supervised learning approaches. While supervised learning has been instrumental in developing the foundational language capabilities of these models, RL offers the potential for the emergence of more advanced, self-improving reasoning skills. This transition, however, necessitates overcoming the inherent difficulties in credit assignment within long sequences of generated text, effectively exploring the high-dimensional space of possible outputs, and precisely defining reward signals that guide the model towards genuine reasoning rather than superficial pattern matching. The success of DeepSeek R1 in achieving high reasoning performance through a training regime heavily reliant on RL underscores the viability of this approach, emphasizing the critical role of innovative RL algorithms and carefully designed training strategies in addressing the complexities associated with teaching machines to think.

## Deep Dive into Group Relative Policy Optimization (GRPO)

At the core of DeepSeek AI's approach to training its reasoning models lies the Group Relative Policy Optimization (GRPO) algorithm. This reinforcement learning technique was the primary method used for both DeepSeek R1-Zero and the subsequent DeepSeek R1. GRPO was initially introduced by the DeepSeek team in their paper focusing on mathematical reasoning, DeepSeekMath.

The GRPO algorithm operates through a series of key steps. First, for each given prompt, the model generates multiple potential output responses by sampling from its current policy, which dictates the probability of different token sequences. Following this, each of these generated responses is evaluated and assigned a reward based on a predefined reward function. This reward function can be rule-based, focusing on criteria such as the accuracy of the final answer or the correct formatting of the response, or it can be based on a more complex reward model that has been trained to assess the quality of the generated text. The core of GRPO lies in its calculation of the "advantage" of each response. Instead of relying on a separate value function as in some other RL methods, GRPO estimates a baseline reward from the group of responses generated for the same prompt. The advantage of each individual response is then calculated relative to this group baseline, often involving a normalization of the rewards within the group. Finally, the model's policy is updated to increase the likelihood of generating responses that received higher advantages and to decrease the likelihood of those with lower advantages. This policy optimization step typically includes a Kullback-Leibler (KL) divergence penalty to ensure that the updated policy does not deviate too drastically from a reference policy, such as the initial policy or a policy from a previous training iteration, thereby promoting stability in the learning process.

When compared to other policy optimization methods, such as Proximal Policy Optimization (PPO), GRPO presents several key differences. GRPO can be seen as a variant of PPO specifically designed to enhance efficiency and reduce the computational resources required for training, particularly memory usage. A significant distinction is that GRPO eliminates the need for a separate "critic" network, or value function model, which is a standard component of PPO. In PPO, the critic network is typically of a similar size to the policy network, adding substantial memory and computational overhead. GRPO, in contrast, estimates the baseline reward directly from the group of generated outputs, thus significantly reducing the memory footprint and computational burden. Furthermore, GRPO directly integrates the KL divergence term into its loss function, whereas PPO often uses the KL divergence as part of the reward signal to penalize deviations from the previous policy. The group-based approach to advantage estimation in GRPO aligns particularly well with the nature of reward model training, where multiple outputs for a single input are often examined and compared to determine a preference or quality score.

The decision by DeepSeek AI to employ GRPO over PPO was likely motivated by GRPO's improved efficiency and its suitability for training large-scale language models, especially for tasks that require complex reasoning. In such tasks, the ability to generate and evaluate multiple response options for a given prompt is often beneficial. PPO's reliance on a critic network, which can be as large as the policy network itself, leads to increased memory and computational demands, making GRPO a more viable option for training extremely large models like DeepSeek R1. GRPO's method of estimating a baseline from a group of responses is also advantageous in scenarios where the quality of an answer is best determined in relation to other potential answers, a common characteristic of complex reasoning problems.

## The Multi-Stage Training Pipeline of DeepSeek R1

The training of DeepSeek R1 involved a carefully designed multi-stage pipeline, indicating a sophisticated approach to optimizing the model's capabilities. This pipeline consisted of four distinct phases, each serving a specific purpose in enhancing the model's reasoning and output quality.

The initial stage was **Cold-Start Supervised Fine-Tuning**. The primary goals of this phase were to prevent the unstable "cold start" often encountered when beginning reinforcement learning directly from a base model and to significantly improve the readability and overall coherence of the model's generated text. To achieve this, DeepSeek AI curated a dataset of thousands of high-quality examples that featured long, detailed Chain-of-Thought (CoT) reasoning. The collection of this data involved several methods, including using few-shot prompting techniques with exemplary long CoT instances, directly prompting existing models to generate thorough answers that included self-reflection and verification steps, gathering outputs from DeepSeek R1-Zero and refining their format for better readability, and employing human annotators to further process and enhance the quality of the data. The impact of this initial SFT phase was substantial. It ensured that the special `<think>` tokens, used to denote the model's reasoning process, were consistently understandable to users. It also effectively addressed the language mixing issues that were prevalent in DeepSeek R1-Zero's outputs and resulted in responses that were more polished, better structured, and adhered to a defined format, typically including a summary of the reasoning process and the final answer. Furthermore, this phase provided a more stable and well-guided starting point for the subsequent reinforcement learning training, potentially leading to improved overall performance compared to initiating RL directly from the base DeepSeek-V3 model.

The second stage was **Reasoning-Oriented Reinforcement Learning**. This phase was specifically focused on enhancing the model's ability to perform complex reasoning, particularly in domains such as coding, mathematics, science, and logical problem-solving, where questions typically have well-defined and verifiable solutions. The primary reward mechanisms employed during this stage were accuracy rewards, which evaluated whether the model's final output was correct, and format rewards, which incentivized the model to structure its reasoning process within designated `<think>` and `<answer>` tags. To address the issue of language mixing that was observed to sometimes re-emerge during this RL phase, a language consistency reward was introduced. This reward was calculated based on the proportion of words in the target language within the Chain-of-Thought reasoning. The final reward signal used to guide the learning process was a direct summation of the accuracy reward and the language consistency reward. The core of this training stage relied on the same large-scale reinforcement learning process, utilizing the GRPO algorithm, that was previously employed in the training of DeepSeek R1-Zero.

The third stage involved **Rejection Sampling and Supervised Fine-Tuning**. In this phase, the researchers expanded the scope of training data and the criteria for evaluating correctness. New training data was collected by sampling from the RL checkpoint obtained at the end of the reasoning-oriented RL stage. For reasoning questions that did not have a single, objectively correct answer, DeepSeek-V3 was employed as a judge to assess the quality and correctness of the model's reasoning. The types of questions curated for fine-tuning were broadened to include both reasoning-based and non-reasoning-based prompts, aiming to improve the model's overall performance across a wider range of tasks. The base model was then further fine-tuned using this newly collected and curated dataset.

The final stage of the training pipeline was **Reinforcement Learning for All Scenarios**. In this phase, a final reinforcement learning run was conducted, but this time it encompassed all types of tasks, including both the reasoning-intensive tasks from earlier stages and more general tasks. The calculation of the advantage within the GRPO algorithm was adjusted to take into account not only the accuracy of the reasoning (using the same rule-based approach as in the reasoning-oriented RL stage) but also human preferences for qualities like helpfulness and harmlessness in the model's responses. This likely involved incorporating feedback from human evaluators or using reward models that were trained to assess these more subjective aspects of language generation. The ultimate goal of this final RL stage was to produce a well-rounded model that not only excelled in complex reasoning tasks but also aligned with user expectations for being both helpful and safe in its interactions.